{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to the image\n",
        "image_path = '/content/74530.png'\n",
        "output_image_path = '/content/74530_with_boxes.png'  # Path to save the output image\n",
        "\n",
        "# Initialize the YOLOv5 model (using a larger model 'yolov5x')\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5x')\n",
        "\n",
        "# Set lower confidence threshold and NMS IoU threshold\n",
        "model.conf = 0.25  # Confidence threshold\n",
        "model.iou = 0.45   # NMS IoU threshold\n",
        "\n",
        "# Read the image\n",
        "img = cv2.imread(image_path)\n",
        "\n",
        "# Check if the image was loaded successfully\n",
        "if img is not None:\n",
        "    # Get original image dimensions\n",
        "    orig_height, orig_width, _ = img.shape\n",
        "\n",
        "    # Convert the image from BGR to RGB format\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Optionally resize the image to 640x640 for inference\n",
        "    img_resized = cv2.resize(img_rgb, (640, 640))\n",
        "\n",
        "    # Perform object detection\n",
        "    results = model(img_resized)\n",
        "\n",
        "    # Print the results for debugging\n",
        "    print(results)  # Check the structure of results\n",
        "\n",
        "    # Extract detected object names and bounding boxes\n",
        "    detections = results.pandas().xyxy[0]\n",
        "    if not detections.empty:\n",
        "        object_names = detections['name'].unique()\n",
        "        print(\"Detected objects:\", object_names)\n",
        "\n",
        "        # Scaling factor between resized image (640x640) and original image\n",
        "        x_scale = orig_width / 640\n",
        "        y_scale = orig_height / 640\n",
        "\n",
        "        # Draw bounding boxes on the original image (RGB format)\n",
        "        for i in range(len(detections)):\n",
        "            x_min, y_min, x_max, y_max = detections.iloc[i][['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "            confidence = detections.iloc[i]['confidence']\n",
        "            class_id = detections.iloc[i]['class']\n",
        "            label = results.names[int(class_id)]\n",
        "\n",
        "            # Scale bounding boxes to match original image size\n",
        "            x_min = int(x_min * x_scale)\n",
        "            y_min = int(y_min * y_scale)\n",
        "            x_max = int(x_max * x_scale)\n",
        "            y_max = int(y_max * y_scale)\n",
        "\n",
        "            # Draw rectangle (bounding box) on the original image\n",
        "            cv2.rectangle(img_rgb, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "\n",
        "            # Adjust the size of the label text\n",
        "            font_scale = 0.5  # Adjust this to change the text size (0.5 is smaller, increase for larger)\n",
        "            font_thickness = 1  # You can also adjust the thickness of the text\n",
        "\n",
        "            # Get the size of the text\n",
        "            text = f'{label} {confidence:.2f}'\n",
        "            (text_width, text_height), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)\n",
        "\n",
        "            # Draw a filled rectangle (red) as a background for the text\n",
        "            cv2.rectangle(img_rgb, (x_min, y_min - text_height - 5),\n",
        "                          (x_min + text_width, y_min), (0, 0, 255), -1)\n",
        "\n",
        "            # Put label text (white color) on top of the red rectangle\n",
        "            cv2.putText(img_rgb, text, (x_min, y_min - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        font_scale, (255, 255, 255), font_thickness)\n",
        "\n",
        "        # Save the original image with bounding boxes (converted back to BGR)\n",
        "        img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
        "        cv2.imwrite(output_image_path, img_bgr)\n",
        "        print(f\"Image saved to {output_image_path}\")\n",
        "\n",
        "        # Display the original image with bounding boxes (in RGB)\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(img_rgb)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No objects detected.\")\n",
        "else:\n",
        "    print(f\"Failed to load image: {image_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RYVlI6_ecm_",
        "outputId": "d8e55bf2-9eab-49a3-b094-5808880b9df7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2024-9-27 Python-3.10.12 torch-2.4.1+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients, 205.5 GFLOPs\n",
            "Adding AutoShape... \n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image 1/1: 640x640 1 person, 1 tie, 2 forks, 1 knife, 1 bowl, 1 pizza, 4 chairs\n",
            "Speed: 13.3ms pre-process, 4004.3ms inference, 1.4ms NMS per image at shape (1, 3, 640, 640)\n",
            "Detected objects: ['knife' 'pizza' 'person' 'fork' 'tie' 'bowl' 'chair']\n",
            "Image saved to /content/74530_with_boxes.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zf4X2x6nypSG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}